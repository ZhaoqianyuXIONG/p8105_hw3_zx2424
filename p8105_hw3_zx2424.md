p8105_hw3_zx2424
================
Zhaoqianyu Xiong
2022-10-15

## Problem 1

``` r
data("instacart")
```

Make a dataframe based on the “instacart” dataset.

``` r
instacart = 
  instacart %>% 
  as_tibble(instacart)
```

The size of the dataset is 1384617\*15. Its variables contain
information relating to id of order, product, user, department and
aisle; name of the product, aisle and department. Also, order in which
each product was added into cart, the order sequence number of the user,
the day of the week on which the order was placed, the hour of the day
on which the order was placed, days of the last order can be learned
from the data. Taking the first row of the dataset for example, a
customer whose id number is 112108 firstly added Bulgarian Yogurt (id =
49302) into the cart, which belongs to yogurt aisle (id = 120) and daily
eggs department (id = 16). The purchase happened on Thursday, 10 am.
Days since the last order of the customer is 9.

There are 134 different aisles. And the most items are ordered from
fresh vegetables.

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Make a plot that shows the number of items ordered in each aisle.

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

Make a table showing three most popular items in each of the aisles.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

| aisle                      | product_name                                  |    n | rank |
|:---------------------------|:----------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered.

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

| product_name     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

## Problem 2

Load, tidy and otherwise wrangle the data.

``` r
accel = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names()  %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "time_minute",
    names_prefix = "activity_",
    values_to = "activity") %>%
  mutate(weekday_vs_weekend = ifelse(day == "Sunday" | day == "Saturday", "weekend", "weekday"),
  time_minute = as.integer(time_minute),
  day = factor(day, levels = str_c(c("Mon", "Tues", "Wednes", "Thurs", "Fri", "Satur", "Sun"), "day"))) %>%
  arrange(day_id, time_minute)
```

    ## Rows: 35 Columns: 1443
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The resulting dataset includes variables “week”, “day_id”, “day”,
“time_minute”, “activity”, “weekday_vs_weekend”. I introduced new
variable “time_minute” to show the exact time (count by minute) of the
recorded activity. This dataset has 50400 observations.

Total activity for each day

``` r
accel %>%
  group_by(day_id) %>%
  summarize(total = sum(activity)) %>%
  arrange(day_id) %>%
  knitr::kable()
```

| day_id |     total |
|-------:|----------:|
|      1 | 480542.62 |
|      2 |  78828.07 |
|      3 | 376254.00 |
|      4 | 631105.00 |
|      5 | 355923.64 |
|      6 | 307094.24 |
|      7 | 340115.01 |
|      8 | 568839.00 |
|      9 | 295431.00 |
|     10 | 607175.00 |
|     11 | 422018.00 |
|     12 | 474048.00 |
|     13 | 423245.00 |
|     14 | 440962.00 |
|     15 | 467420.00 |
|     16 | 685910.00 |
|     17 | 382928.00 |
|     18 | 467052.00 |
|     19 | 371230.00 |
|     20 | 381507.00 |
|     21 | 468869.00 |
|     22 | 154049.00 |
|     23 | 409450.00 |
|     24 |   1440.00 |
|     25 | 260617.00 |
|     26 | 340291.00 |
|     27 | 319568.00 |
|     28 | 434460.00 |
|     29 | 620860.00 |
|     30 | 389080.00 |
|     31 |   1440.00 |
|     32 | 138421.00 |
|     33 | 549658.00 |
|     34 | 367824.00 |
|     35 | 445366.00 |

The trend is not apparent for total activity for each day.

When making the plot for 35 days separately using geom_line(), the plot
is too messy and tendency cannot be seen clearly.

``` r
accel %>%
  ggplot(aes(x = time_minute, y = activity, group = day_id, color = day)) +
  geom_line() +
  labs(
    title = "24_hour activity time courses for each day",
    x = "the xth minute of the day",
  )
```

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

Therefore, I use geom_smooth() to make the plot for different days of
the week. It can be concluded that for all the days in the week,
activity was lower at night (from 0th to 400th minute), perhaps because
of sleeping. In the daytime, activities of most days fluctuated slightly
except Sunday and Friday. For Friday, average activity reached a high
level in the evening (from 1200th to 1300th) minute; While it arrived at
its peak in the morning (between 500th to 700th minute) on Sunday.

``` r
accel %>%
  ggplot(aes(x = time_minute, y = activity, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "24_hour activity time courses for each day",
    x = "the xth minute of the day",
  )
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

## Problem 3

``` r
library(p8105.datasets)
data("ny_noaa")

ny_noaa
```

    ## # A tibble: 2,595,176 × 7
    ##    id          date        prcp  snow  snwd tmax  tmin 
    ##    <chr>       <date>     <int> <int> <int> <chr> <chr>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA <NA>  <NA> 
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA <NA>  <NA> 
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA <NA>  <NA> 
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA <NA>  <NA> 
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA <NA>  <NA> 
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA <NA>  <NA> 
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA <NA>  <NA> 
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA <NA>  <NA> 
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA <NA>  <NA> 
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA <NA>  <NA> 
    ## # … with 2,595,166 more rows

The size of original dataset is 2595176\*7. Key variables include
weather station ID, date of observation, precipitation (tens of mm),
snowfall (mm), snow depth (mm), maximum temperature (tens of degrees C),
and minimum temperature (tens of degree C). A great amount of data is
missing for this dataset, reaching 3387623.

Data cleaning. The original units for temperature is tens of degree C,
which is not commonly used. So I converted it into degree C. On the
other hand, because the units for precipitation(mm) and snowfall(mm) are
reasonable, I left them unchanged.

``` r
ny_noaa_tidy = 
ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = as.integer(month)) %>%
  mutate(year = as.integer(year)) %>%
  mutate(day = as.integer(day)) %>%
  mutate(tmax = as.integer(tmax)/10) %>%
  mutate(tmin = as.integer(tmin)/10)
```

The most commonly observed value for snowfall is 0. Because it does not
snow for most days of a year in New York. Therefore, no snowfall is
mostly commonly observed.

``` r
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

common_snow = getmode(pull(ny_noaa, snow))
c(common_snow)
```

    ## [1] 0

Make a plot to show the average max temperature in January and in July
across the year.

``` r
mean_tmax = 
  ny_noaa_tidy %>%
  filter(month == 1 | month == 7) %>%
  select(id, year, month, tmax) %>%
  group_by(id, year, month) %>%
  summarize(average_tmax = mean(tmax, na.rm = TRUE)) 
```

    ## `summarise()` has grouped output by 'id', 'year'. You can override using the
    ## `.groups` argument.

``` r
mean_tmax %>%
  na.omit() %>%
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_line() +
  facet_grid(~month) +
  labs(
    title = "Average max temperature across the years",
    x = "year",
    y = "average max temperature"
  )
```

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />
Average max temperature in January fluctuated around 0 degree. It
firstly increased from 1980 to 1990, then went down until 1995. After
that, the temperature fluctuates up and down in a five-year cycle.
Comparing to that in January, the average max temperature change in July
is smaller, almost all are around 25 to 30 degree C. There are some
outliers in both average max temperature in January and July. For
example, the temperature station whose id is USC00308962 reported the
average max temperature at 14 degree C in July on 1988, while other
stations at the same time reported the average max temperature over 20
degree C.

1)  tmax vs tmin for the full dataset. It can be learned from the plot
    that maximum temperature is mostly distributed between 0 and 30
    degree C, while the minimum temperature is mainly between -15 and 15
    degree C.

``` r
ny_noaa_tidy %>%
  select(tmax, tmin) %>%
  na.omit() %>%
  pivot_longer(
    tmax:tmin,
    names_to = "observation",
    values_to = "temperature") %>%
  ggplot(aes(x = temperature, fill = observation)) +
  geom_density(alpha = .5) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(title = "tmax vs tmin")
```

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

2)  Make a plot to show the distribution of snowfall values greater than
    0 and less than 100 separately by year. It can be learned from the
    plot that most common snowfall is between 0 to 30mm. Heavy snow more
    than 60mm is rare in New York. In terms of the years, snowfall has
    shown a decreasing trend in these years.

``` r
ny_noaa_tidy %>%
  select(year, snow) %>%
  filter(snow > 0 & snow < 100) %>%
  group_by(year) %>%
  na.omit() %>%
  ggplot(aes(x = snow, group = year, color = year)) +
  geom_density(alpha = .5) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(title = "Distribution of snowfall values (0~100) by year",
       x = "snowfall(mm)")
```

<img src="p8105_hw3_zx2424_files/figure-gfm/unnamed-chunk-16-1.png" width="90%" />
