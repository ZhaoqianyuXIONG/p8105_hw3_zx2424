---
title: "p8105_hw3_zx2424"
author: "Zhaoqianyu Xiong"
date: "2022-10-12"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r}
data("instacart")
```

The size of the dataset is 1384617*15.
```{r}
instacart = 
  instacart %>% 
  as_tibble(instacart)
```

There are 134 different aisles. And the most items are ordered from fresh vegetables.
```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Make a plot that shows the number of items ordered in each aisle.
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60))
```

Make a table showing three most popular items in each of the aisles.
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered.
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

## Problem 2
Load, tidy and otherwise wrangle the data. 
```{r}
accel = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names()  %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "time_minute",
    names_prefix = "activity_",
    values_to = "activity") %>%
  mutate(weekday_vs_weekend = ifelse(day == "Sunday" | day == "Saturday", "weekend", "weekday"),
  time_minute = as.integer(time_minute),
  day = factor(day, levels = str_c(c("Mon", "Tues", "Wednes", "Thurs", "Fri", "Satur", "Sun"), "day"))) %>%
  arrange(day_id, time_minute)
```
The resulting dataset includes variables "week", "day_id", "day", "time_minute", "activity", "weekday_vs_weekend". I introduced new variable "time_minute" to show the exact time (count by minute) of the recorded activity. This dataset has `r nrow(accel)` observations.

Total activity for each day
```{r}
accel_day = 
  group_by(accel, day_id) %>%
  summarize(total = sum(activity)) 

accel_day %>%
  ggplot(aes(x = day_id, y = total)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Total activity for each day",
    x = "id of the day",
    y = "total activity"
  )
```
Although there are some extreme points, the trend is that total activity for each day increased for day 1 to day 14, and then went down until day 25. In the last 10 days of observation, it grew again.

When making the plot for 35 days separately using geom_line(), the plot is too messy and tendency cannot be seen clearly. 
```{r}
accel %>%
  ggplot(aes(x = time_minute, y = activity, group = day_id, color = day)) +
  geom_line() +
  labs(
    title = "24_hour activity time courses for each day",
    x = "the xth minute of the day",
  )
```

Therefore, I use geom_smooth() to make the plot for different days of the week. It can be concluded that for all the days in the week, activity was lower at night (from 0th to 400th minute), perhaps because of sleeping. In the daytime, activities of most days fluctuated slightly except Sunday and Friday. For Friday, average activity reached a high level in the evening (from 1200th to 1300th) minute; While it arrived at its peak in the morning (between 500th to 700th minute) on Sunday.
```{r}
accel %>%
  ggplot(aes(x = time_minute, y = activity, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "24_hour activity time courses for each day",
    x = "the xth minute of the day",
  )
```

## Problem 3
```{r}
library(p8105.datasets)
data("ny_noaa")

ny_noaa
```
The size of original dataset is 2595176*7. Key variables include weather station ID, date of observation, precipitation (tens of mm), snowfall (mm), snow depth (mm), maximum temperature (tens of degrees C), and minimum temperature (tens of degree C). A great amount of data is missing for this dataset, reaching `r sum(is.na(ny_noaa))`.

Data cleaning. The original units for temperature is tens of degree C, which is not commonly used. So I converted it into degree C. On the other hand, because the units for precipitation(mm) and snowfall(mm) are reasonable, I left them unchanged.
```{r}
ny_noaa_tidy = 
ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = as.integer(month)) %>%
  mutate(year = as.integer(year)) %>%
  mutate(day = as.integer(day)) %>%
  mutate(tmax = as.integer(tmax)/10) %>%
  mutate(tmin = as.integer(tmin)/10)
```

The most commonly observed value for snowfall is 0. Because it does not snow for most days of a year in New York. Therefore, no snowfall is mostly commonly observed.
```{r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

common_snow = getmode(pull(ny_noaa, snow))
c(common_snow)
```

Make a plot to show the average max temperature in January and in July across the year.
```{r}
mean_tmax = 
  ny_noaa_tidy %>%
  filter(month == 1 | month == 7) %>%
  select(id, year, month, tmax) %>%
  group_by(id, year, month) %>%
  summarize(average_tmax = mean(tmax, na.rm = TRUE)) 

mean_tmax %>%
  na.omit() %>%
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_line() +
  facet_grid(~month) +
  labs(
    title = "Average max temperature across the years",
    x = "year",
    y = "average max temperature"
  )
```
Average max temperature in January fluctuated around 0 degree. It firstly increased from 1980 to 1990, then went down until 1995. After that, the temperature fluctuates up and down in a five-year cycle.
Comparing to that in January, the average max temperature change in July is smaller, almost all are around 25 to 30 degree C. 
There are some outliers in both average max temperature in January and July. For example, the temperature station whose id is USC00308962 reported the average max temperature at 14 degree C in July on 1988, while other stations at the same time reported the average max temperature over 20 degree C.

i) tmax vs tmin for the full dataset.
It can be learned from the plot that maximum temperature is mostly distributed between 0 and 30 degree C, while the minimum temperature is mainly between -15 and 15 degree C.
```{r}
ny_noaa_tidy %>%
  select(tmax, tmin) %>%
  na.omit() %>%
  pivot_longer(
    tmax:tmin,
    names_to = "observation",
    values_to = "temperature") %>%
  ggplot(aes(x = temperature, fill = observation)) +
  geom_density(alpha = .5) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(title = "tmax vs tmin")

``` 

ii) Make a plot to show the distribution of snowfall values greater than 0 and less than 100 separately by year.
It can be learned from the plot that most common snowfall is between 0 to 30mm. Heavy snow more than 60mm is rare in New York. In terms of the years, snowfall has shown a decreasing trend in these yearsã€‚
```{r}
ny_noaa_tidy %>%
  select(year, snow) %>%
  filter(snow > 0 & snow < 100) %>%
  group_by(year) %>%
  na.omit() %>%
  ggplot(aes(x = snow, group = year, color = year)) +
  geom_density(alpha = .5) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(title = "Distribution of snowfall values (0~100) by year",
       x = "snowfall(mm)")
```
